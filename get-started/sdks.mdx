---
title: Client SDKs

icon: terminal

description: We provide client libraries in a number of popular languages that make it easier to work with the PremAI API.
---

<Note>
We give you the option to use the [Prem AI API](/api-reference/introduction), the PremAI SDK, or the OpenAI SDK.

</Note>

<Note>
Before using the SDKs, you'll need to create an API key. See our [API Key guide](/api-reference/api-key) for instructions.
</Note>

## Use with PremAI SDK

### Install the PremAI SDK
<CodeGroup>
```bash javascript/typescript
npm install premai
```
```bash python
pip install premai
```
</CodeGroup>

### List Models
<CodeGroup>
```javascript javascript/typescript
import PremAI from 'premai';

const client = new PremAI({
  base_url: "https://studio.premai.io/",
  apiKey: process.env['PREMAI_API_KEY'], // This is the default and can be omitted
});

const response = await client.models.list();

console.log(response.data);
```
```python python
import os
from premai import PremAI

client = PremAI(
    base_url="https://studio.premai.io/",
    api_key=os.environ.get("PREMAI_API_KEY"),  # This is the default and can be omitted
)
response = client.models.list()
print(response.data)
```
</CodeGroup>

### Chat Completions
import PremChatCompletions from '/snippets/prem-chat-completions.mdx';

<PremChatCompletions/>

### Chat Completion with Streaming

<Warning>
Streaming is not supported for the PremAI SDK. Will be supported in the future.
</Warning>


## Use with OpenAI SDK

### Install the OpenAI SDK
<CodeGroup>
```bash javascript/typescript
npm install openai
```
```bash python
pip install openai
```
</CodeGroup>

### List Models
<CodeGroup>
```javascript javascript/typescript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: "https://studio.premai.io/api/v1/",
  apiKey: process.env['PREMAI_API_KEY'], // This is the default and can be omitted
});

const response = await client.models.list();

console.log(response.data);
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),  # This is the default and can be omitted
)
response = client.models.list()
print(response.data)
```
</CodeGroup>

### Chat Completions
<Note>
The model name can be replaced with the name of your fine-tuned models as well.
</Note>
<CodeGroup>
```javascript javascript/typescript
import OpenAI from "openai";

const client = new OpenAI({
    baseURL: "https://studio.premai.io/api/v1/",
    apiKey: process.env.PREMAI_API_KEY,
});

//Create a chat completion
const response = await client.chat.completions.create({
    model: "llama3.2-3b", //Or any other model you want to use
    messages: [{ role: "user", content: "Write a one-sentence bedtime story about a unicorn." }]
});

console.log(response.choices[0].message.content);
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create a chat completion
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
)

print(response.choices[0].message.content)
```
</CodeGroup>

### Chat Completion with Streaming
<CodeGroup>
```javascript javascript/typescript
import OpenAI from "openai";

const client = new OpenAI({
    baseURL: "https://studio.premai.io/api/v1/",
    apiKey: process.env.PREMAI_API_KEY,
});

//Create a chat completion
const response = await client.chat.completions.create({
    model: "llama3.2-3b", //Or any other model you want to use
    messages: [{ role: "user", content: "Write a one-sentence bedtime story about a unicorn." }],
    stream: true,
});

for await (const chunk of response) {
    process.stdout.write(chunk.choices[0].delta.content);
}
```
```python python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://studio.premai.io/api/v1/",
    api_key=os.environ.get("PREMAI_API_KEY"),
)

# Create completion
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Who won the world series in 2020?"}],
    model="llama3.2-3b", # Or any other model you want to use
    stream=True,
)

for chunk in response:
    print(chunk.choices[0].delta.content, end='', flush=True)
```
</CodeGroup>
